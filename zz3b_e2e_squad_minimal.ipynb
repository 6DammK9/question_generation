{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pipelines import pipeline\n",
    "from nlgeval import compute_metrics\n",
    "from tqdm import tqdm\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\lauts\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 57.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset = load_dataset('squad', split=['train', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"question-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = 'eval_test/hyp.txt'\n",
    "ref = 'eval_test/ref1.txt'\n",
    "ctx = 'eval_test/ref2.txt'\n",
    "res = 'eval_test/res.txt'\n",
    "\n",
    "dev_mode = True\n",
    "squad_size = 100 if dev_mode else len(train_dataset)\n",
    "\n",
    "#corpus = [text, text2,text3, text4]\n",
    "c_t = None #Current Title\n",
    "c_q = [] #Current QG set\n",
    "cqc = \"\" #Current concatenated questions\n",
    "ccc = \"\" #Current context\n",
    "\n",
    "h_q = [] #Lines of predicted question (concated by each topic)\n",
    "r_q = [] #Lines of actual question (concated by each topic)\n",
    "c_c = [] #Lines of context (for each topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wq(ta, xt=hyp):\n",
    "    with open(xt, 'w', encoding='utf-8') as f:\n",
    "        for t in tqdm(ta):\n",
    "            nt = nlp(t)\n",
    "            f.writelines([' '.join(nt), '\\n'])\n",
    "\n",
    "def wc(ta, xt=ref):\n",
    "    with open(xt, 'w', encoding='utf-8') as f:\n",
    "        f.writelines('\\n'.join(ta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QG for 100 records: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:40<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct context found: 17\n",
      "Writing eval_test/hyp.txt...\n",
      "Writing eval_test/ref1.txt...\n",
      "Writing eval_test/ref2.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"QG for {0} records: \".format(squad_size))\n",
    "\n",
    "for i in tqdm(range(0, squad_size)):\n",
    "    t_d = train_dataset[i]\n",
    "    #tdt = t_d[\"title\"]\n",
    "\n",
    "    #Fill in first context\n",
    "    if i == 0:\n",
    "        ccc = t_d[\"context\"]\n",
    "        #c_t = tdt\n",
    "\n",
    "    #Force write result when it reaches the end\n",
    "    if i == squad_size:\n",
    "        ccc = \"\"\n",
    "\n",
    "    #print(len(ccc), len(t_d[\"context\"]))\n",
    "\n",
    "    #Skip if no context swap\n",
    "    if t_d[\"context\"] == ccc:\n",
    "        cqc = cqc + \"{} \".format(t_d[\"question\"])\n",
    "        if i < squad_size - 1:\n",
    "            continue\n",
    "    \n",
    "    #Title has been swapped. Retrieve predicted questions\n",
    "    c_q = nlp(ccc)\n",
    "\n",
    "    h_q.append(' '.join(cq[\"question\"] for cq in c_q))\n",
    "    r_q.append(cqc)\n",
    "    c_c.append(ccc)\n",
    "    \n",
    "    #Swap context\n",
    "    ccc = t_d[\"context\"]\n",
    "    #Clear question segment\n",
    "    cqc = \"\"\n",
    "\n",
    "\n",
    "print(\"Distinct context found: {0}\".format(len(hyp)))\n",
    "print(\"Writing {0}...\".format(hyp))\n",
    "wc(h_q, xt=hyp)\n",
    "print(\"Writing {0}...\".format(ref))\n",
    "wc(r_q, xt=ref)\n",
    "print(\"Writing {0}...\".format(ctx))\n",
    "wc(c_c, xt=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu_1: 0.630316\n",
      "Bleu_2: 0.455492\n",
      "Bleu_3: 0.337768\n",
      "Bleu_4: 0.257436\n",
      "METEOR: 0.231376\n",
      "ROUGE_L: 0.347077\n",
      "CIDEr: 0.145509\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = compute_metrics(hypothesis=hyp, references=[ref, ctx], no_skipthoughts=True, no_glove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing result to eval_test/res.txt...\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing result to {0}...\".format(res))\n",
    "      \n",
    "json_res = json.dumps(metrics_dict, indent = 4) \n",
    "with open(res, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(json_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47498515ab12efa1b601cdba97f63393ba71a785e03ed8d0031866b5c777e044"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
